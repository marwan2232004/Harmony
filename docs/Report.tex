\documentclass[a4paper,12pt]{article}
\usepackage{float}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{tikz}
\usetikzlibrary{positioning, calc}

\definecolor{backcolour}{RGB}{245,245,244}
\definecolor{codegreen}{RGB}{34,139,34}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{128,0,128}
\definecolor{codeblue}{RGB}{0,0,128}

\lstdefinestyle{pseudocode}{
    backgroundcolor=\color{backcolour},       % light gray background
    basicstyle=\ttfamily\small,               % small monospaced font
    commentstyle=\color{codegreen}\itshape,   % comments in green italics
    keywordstyle=\color{codeblue}\bfseries,   % keywords in bold blue
    numberstyle=\tiny\color{codegray},        % line numbers in tiny gray
    stringstyle=\color{codepurple},           % string literals in purple
    breakatwhitespace=true,                  % allow breaks mid-line
    breaklines=true,                          % automatic line breaking
    captionpos=b,                             % captions at bottom
    keepspaces=true,                          % preserve spaces
    numbers=left,                             % show line numbers on the left
    numbersep=5pt,                            % gap between numbers and code
    showspaces=false,                         % don’t mark spaces
    showstringspaces=false,                   % don’t mark spaces in strings
    showtabs=false,                           % don’t mark tabs
    tabsize=2,                                % tabs = 2 spaces
    moredelim=[l][\color{codegreen}\itshape]{//}, % lines starting ‘//’ are comments
    literate={-}{{\text{-}}}{1},                        % prevent line breaks athyphens
    moredelim=[l][\color{codegreen}\itshape]{//}
}

% then in your document:
\lstset{style=pseudocode}

\geometry{margin=1in}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{1cm}

    \includegraphics[width=0.98\textwidth]{image.png}

    \vfill
    
    \textbf{\Huge Neural Network}\\
    \textbf{\Large Project \\ Speaker Gender Age  Recognition}\\
    
    \vfill

    \textbf{Submitted by:}\\
    Kareem Mohamed \quad (ID: 9220600) \\
    Moamen Hefny  \quad (ID: 9220886) \\
    Marwan Mohamed \quad (ID: 9220808) \\
    Youssef Tarek \quad (ID: 9220990) \\
    
    \vfill
    
    \textbf{Spring 2025}
    
\end{titlepage}

\tableofcontents
\newpage

\section{Preprocessing}

The preprocessing stage was responsible for preparing raw audio files before feature extraction and training. To handle this, we developed a pipeline built around the \texttt{Essentia} library, which provided a solid foundation for reading and processing audio data.

\subsection{Audio Loading and Validation}

Each audio file was first checked for existence and size to avoid corrupt or empty files. Using Essentia’s \texttt{MonoLoader}, the audio was converted to mono format and loaded into memory. Along with the raw waveform, we also extracted the sample rate and computed the duration of the clip. Files that failed any check were automatically skipped to maintain dataset quality.

\subsection{Preprocessing Options}

Our preprocessing pipeline included several configurable operations, which could be toggled on or off via command-line arguments. These steps were applied in the following order:

\begin{itemize}
    \item \textbf{Trimming:} Clips were trimmed or padded to a fixed target duration (default 4 seconds). This ensured that all input samples were of uniform length, which is essential for consistent feature extraction.
    
    \item \textbf{Volume Normalization:} Each clip was normalized to a target RMS level (default 0.2), allowing for consistent loudness across the dataset.
    
    \item \textbf{Noise Reduction:} Although implemented, this option was disabled by default to avoid distorting clean audio. It could be enabled to suppress background noise below a given threshold.
    
    \item \textbf{Silence Removal:} Segments of silence were detected and removed based on a minimum silence duration (default 500ms) and a threshold level (default 0.01). This helped in focusing the model on actual speech content.
\end{itemize}

Each of these steps could be individually disabled using flags such as \texttt{--no-trim}, \texttt{--no-normalize}, \texttt{--no-noise-reduction}, and \texttt{--no-silence-removal}.

\subsection{Batch Processing and Dataset Handling}

To handle large datasets efficiently, we implemented support for batch processing using a TSV file. This file contained paths to audio samples and optional metadata. We could specify a range of lines to process (e.g., start and end line), a maximum number of files, and the output directory.

\newpage
\section{Cleaning Dataset}

Before training, the dataset underwent a cleaning phase to ensure that all entries were valid and usable. This step focused on filtering out problematic files, reducing noise in metadata, and preparing a balanced subset for training.

\subsection{Metadata Filtering and Validation}

We built a metadata cleaning tool to process the original TSV file associated with the dataset. This tool dropped irrelevant columns, removed rows that pointed to missing or unreadable audio files, and retained only entries that could be successfully loaded and analyzed.

For each entry, the audio file was checked for existence and duration. Only those with a valid path and a positive duration were kept. The cleaned metadata was saved in a new TSV file with just the filename, age group, gender, and duration.

\subsection{Configurable Cleaning Script}

The cleaning process was configurable through command-line arguments. Key parameters included:
\begin{itemize}
    \item \texttt{--dataset-path=<path>}: Specifies the directory containing the audio files.
    \item \texttt{--metadata-file=<file>}: Points to the input metadata TSV file.
    \item \texttt{--clean-tsv}: Enables the metadata validation and cleaning step.
\end{itemize}

\subsection{Categorization by Gender and Age Group}

After filtering, the entries were categorized based on gender and age group. The target categories were set to \texttt{male} and \texttt{female} for gender, and \texttt{twenties} and \texttt{fifties} for age groups. Each audio entry was mapped into its corresponding category based on these two labels.

\subsection{Exporting a Balanced Dataset}

To ensure balanced training data, the user was prompted to enter the number of samples per category to include in the final dataset. For each category, the tool randomly selected the desired number of samples (shuffling the data beforehand) and saved the resulting subset to a new TSV file.

The exported file contained one entry per line in the format: filename, age, gender, and duration. All categories were kept balanced with the same number of entries.

\newpage
\section{Feature Extraction}
\label{sec:feature-extraction}

To convert audio data into a structured form suitable for machine learning, we implemented a robust feature extraction pipeline. This system supports multiple types of acoustic features commonly used in speech processing. After extensive experimentation with different feature sets and their combinations, we found that MFCCs (Mel-Frequency Cepstral Coefficients) with 26 coefficients yielded the highest classification accuracy.

\subsection*{Supported Feature Types}

Our feature extraction module is capable of computing the following acoustic features:

\begin{itemize}
    \item \textbf{MFCC (Mel-Frequency Cepstral Coefficients)}: Captures the timbral texture of audio; both mean and standard deviation across frames can be computed.
    \item \textbf{Chroma Features}: Represents the energy distribution across 12 pitch classes (e.g., C, C\#, D, etc.) and is useful for capturing harmonic content.
    \item \textbf{Spectral Contrast}: Measures the difference in energy between spectral peaks and valleys across frequency bands.
    \item \textbf{Tonnetz Features}: Based on harmonic relationships and tonal centroid features.
    \item \textbf{Mel Spectrogram}: Represents power across Mel-scaled frequency bands and captures temporal-spectral information.
\end{itemize}

\subsection*{Feature Selection and Performance}

We systematically evaluated the effectiveness of each feature type, as well as several combinations. The table below summarizes the supported features and whether they were ultimately selected for model training:

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Feature Type} & \textbf{Supported} & \textbf{Selected for Final Model} \\
\hline
MFCC (26 coefficients)      & Yes & \textbf{Yes} \\
Chroma                      & Yes & No \\
Spectral Contrast           & Yes & No \\
Tonnetz                     & Yes & No \\
Mel Spectrogram             & Yes & No \\
\hline
\end{tabular}
\caption{Summary of supported features and final selection}
\label{tab:feature-selection}
\end{table}

\subsection*{Pipeline Details}

The pipeline reads metadata from a TSV file, processes the corresponding audio files, extracts the selected features, and saves them to new TSV files for training and testing. The system is designed to be configurable via command-line arguments, and supports options such as train/test split ratio, random seed initialization, and output directory specification.

Progress tracking and error reporting are included to enhance usability and transparency. In cases where an audio file is missing or unreadable, the pipeline logs the error without halting execution.

\newpage
\section{Model Training}
\label{sec:model-training}

\subsection{Classifier Overview}
The stacking classifier combines predictions from multiple base learners via a meta‐learner to improve overall accuracy.  
\begin{itemize}
  \item \textbf{Base learners:} A set of individual classifiers trained on the original feature space.
  \item \textbf{Meta‐learner:} A classifier trained on out‐of‐fold predictions of the base learners.
  \item \textbf{Cross‐validation:} K‐fold splitting to generate out‐of‐fold predictions without data leakage.
\end{itemize}

\subsection{Algorithm Pseudocode}
\lstset{
  literate={-}{{-}}1
}
\begin{lstlisting}
// Inputs: feature matrix X (N×D), labels y (N), 
//         base models B[1..L], meta_model M, folds K
Shuffle indices 0..N-1
Partition indices into K folds
Initialize Z[N][L] = 0

// Generate out_of_fold predictions
for l = 1 to L parallel:                // train each base in parallel
  for fold = 1 to K:
    train_idx = all indices not in fold
    test_idx  = indices in fold

    B[l].train( X[train_idx], y[train_idx] )
    ŷ = B[l].predict( X[test_idx] )
    for i in test_idx:
      Z[i][l] = ŷ[i]

// Train meta_learner
M.train( Z, y )

// Retrain each base on full data
for l = 1 to L:
  B[l].train( X, y )
\end{lstlisting}

\subsection{Base Models Comparison}
The following table summarizes all supported base learners and which combination was selected in the final system.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|l|}
\hline
\textbf{Model} & \textbf{Supported} & \textbf{Used} & \textbf{Notes} \\
\hline
Support Vector Machine (RBF) & Yes & \textbf{Yes}  & High margin separation, tuned C and $\gamma$ \\
Random Forest                & Yes & No            & 700 trees, similar accuracy but slower \\
Extra Trees                  & Yes & No            & 400 trees, fast but no gain over RF \\
k-Nearest Neighbors (KNN)    & Yes & \textbf{Yes}  & \(k=5\), Euclidean distance \\
Neural Network (2-layer)     & Yes & No            & 64+32 hidden units, prone to overfitting \\
\hline
\end{tabular}
\caption{Base learners supported and final selection}
\label{tab:base-models}
\end{table}

\subsection{Configuration and Saving}
\begin{itemize}
  \item \texttt{--n-folds}: number of CV folds (default 5).
  \item \texttt{--seed}: random seed for reproducibility.
  \item \texttt{--train-path}, \texttt{--test-path}: feature TSV inputs.
  \item Models and configuration are saved to a directory with subfolders per base learner and a \texttt{config.txt} recording parameters.
\end{itemize}

\newpage
\section{Inference Pipeline}
\label{sec:inference}

This stage runs the trained stacking classifier on a directory of test audio files, producing predicted labels and evaluation metrics.

\subsection{Initialization and Configuration}
The executable accepts the following command-line options:
\begin{itemize}
  \item \texttt{--data-dir=<path>}: directory containing test audio files (\texttt{.wav} or \texttt{.mp3}).
  \item \texttt{--model-dir=<path>}: directory where the trained models and \texttt{summary.txt} reside.
  \item \texttt{--ground-truth=<file>}: TSV file with true labels for accuracy reporting.
  \item \texttt{--mode=<single|combined>}:  
    \begin{itemize}
      \item \texttt{single}: use one stacking model for both gender and age combined.  
      \item \texttt{combined}: load separate gender and age classifiers.
    \end{itemize}
  \item \texttt{--gender-prefix}, \texttt{--age-prefix}: prefixes for subdirectories when \texttt{mode=combined}.
\end{itemize}

\subsection{Model Loading}
\begin{enumerate}
  \item Parse \texttt{summary.txt} to retrieve hyperparameters (SVM C, gamma, KNN k, etc.).
  \item Instantiate base learners (SVM, KNN) and the logistic regression meta-learner.
  \item Call \texttt{loadModels()} on the appropriate directory or subdirectories.
\end{enumerate}

\subsection{Test File Discovery}
All non-empty files with \texttt{.wav} or \texttt{.mp3} extensions in \texttt{data-dir} are collected. Empty or unreadable files are skipped.

\subsection{Feature Extraction}
\begin{itemize}
  \item For each file, run the audio preprocessor (trimming and normalization disabled).
  \item Extract MFCC feature vectors using the same parameters as during training.
  \item Accumulate feature vectors into an Eigen matrix \(\mathbf{X}\).
\end{itemize}

\subsection{Prediction}
\begin{enumerate}
  \item If \texttt{mode=combined}, separately predict gender and age, then combine into a single class code (\( \text{class} = 2 \times \text{ageCode} + \text{genderCode}\)).
  \item If \texttt{mode=single}, predict directly from the combined model.
  \item Store the vector of predicted class indices.
\end{enumerate}

\subsection{Output and Evaluation}
\begin{itemize}
  \item Write predicted class indices to \texttt{results.txt} (one per line).
  \item Record total inference time in \texttt{time.txt}.
  \item If a ground-truth TSV is provided, compare predictions to true labels:
    \begin{itemize}
      \item Generate \texttt{accuracy.txt} with one line per file showing filename, true label, predicted label, and “Correct”/“Wrong” flag.
      \item Log overall accuracy percentage.
    \end{itemize}
\end{itemize}


\end{document}